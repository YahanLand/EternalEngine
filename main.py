# main.py
import subprocess  # This module allows us to run external commands (like our Ollama CLI) from Python.

def get_llm_response(prompt):
    """
    Sends the given prompt to the local model via Ollama and returns the response.
    
    Parameters:
        prompt (str): The text input from the user.
    
    Returns:
        str: The response generated by the local model.
    """
    try:
        # Run the 'ollama chat' command with the user's prompt.
        # The list passed to subprocess.run contains the command and its arguments.
        # 'capture_output=True' captures the output of the command.
        # 'text=True' tells Python to treat the output as a string (rather than bytes).
        # 'check=True' will raise an error if the command fails.
        result = subprocess.run(
            ["ollama", "run", "llama3.1:8b", prompt],
            capture_output=True,
            text=True,
            encoding='utf-8',  # Specify UTF-8 explicitly
            check=True
        )
        # The output from the command is stored in result.stdout.
        return result.stdout.strip()  # .strip() removes any extra whitespace/newlines.
    except subprocess.CalledProcessError as e:
        # If an error occurs while running the command, we catch it here.
        print("Error communicating with the local model:", e)
        print("Standard Error Output:", e.stderr)
        return "Sorry, I couldn't get a response from the model."

def main():
    """
    Main function that runs the chat loop.
    """
    print("Welcome to AI avatar platform!")
    print("Type 'exit' or 'quit' to end the session.\n")
    
    # Start an infinite loop to keep the conversation going until the user exits.
    while True:
        # Get input from the user.
        user_input = input("You: ")
        
        # Check if the user wants to exit the chat.
        if user_input.lower() in ["exit", "quit"]:
            print("Exiting the chat. Goodbye!")
            break
        
        # Send the user's input to the model and get the response.
        response = get_llm_response(user_input)
        
        # Print the model's response.
        print("Model:", response)
        print()  # Print an empty line for readability.

# This ensures that main() is called when the script is run directly.
if __name__ == '__main__':
    main()

